"""
AI-Generated Text Detector (v2 — multi-signal approach).

Combines three categories of detection signals to estimate whether a
document was written by a human or generated by an AI model:

Category 1 — AI Linguistic Markers  (50% of final score)
    Detects overuse of transition words, formulaic/hedging phrases,
    discourse connectives, and repetitive sentence openers that are
    hallmarks of LLM-generated text.

Category 2 — Statistical Patterns   (30% of final score)
    Improved burstiness, vocabulary richness (MATTR), sentence-length
    uniformity, paragraph regularity, and sentence complexity measures.

Category 3 — Corpus Similarity      (20% of final score)
    TF-IDF cosine similarity and n-gram overlap against a hidden
    reference corpus of known AI-generated texts.

Each feature produces a 0–1 sub-score where **higher = more likely AI**.
The final score is a weighted average mapped to 0–100%.
"""

import math
import re
import string
from collections import Counter

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity as sklearn_cosine


# ── regex helpers ────────────────────────────────────────
_SENT_SPLIT = re.compile(r'(?<=[.!?])\s+')
_WORD_RE = re.compile(r"[a-zA-Z']+")
_PARA_SPLIT = re.compile(r'\n\s*\n')


def _sentences(text: str) -> list[str]:
    """Split text into rough sentences."""
    parts = _SENT_SPLIT.split(text.strip())
    return [s.strip() for s in parts if len(s.strip()) > 5]


def _words(text: str) -> list[str]:
    """Extract lowercase alphabetic words from text."""
    return [w.lower() for w in _WORD_RE.findall(text)]


# ═══════════════════════════════════════════════════════════
#  CATEGORY 1 — AI Linguistic Markers  (50 %)
# ═══════════════════════════════════════════════════════════

# Common AI transition phrases and discourse markers
_TRANSITION_WORDS = frozenset({
    "furthermore", "moreover", "additionally", "consequently",
    "nevertheless", "nonetheless", "therefore", "thus", "hence",
    "subsequently", "accordingly", "meanwhile", "similarly",
    "conversely", "alternatively", "significantly", "ultimately",
    "notably", "importantly", "specifically", "essentially",
    "fundamentally", "increasingly", "particularly",
})

# Formulaic phrases that LLMs love to produce
_FORMULAIC_PHRASES = [
    "it is important to note",
    "it is worth noting",
    "it is essential to",
    "plays a crucial role",
    "plays a vital role",
    "plays a significant role",
    "plays an important role",
    "in today's world",
    "in the modern world",
    "in recent years",
    "in recent decades",
    "cannot be overstated",
    "it is important to acknowledge",
    "it is important to recognize",
    "it is crucial to",
    "in conclusion",
    "to summarize",
    "in summary",
    "as a result",
    "on the other hand",
    "from this perspective",
    "this approach has",
    "this is particularly",
    "these developments have",
    "these changes have",
    "these advancements have",
    "have led to significant",
    "has led to significant",
    "have become increasingly",
    "has become increasingly",
    "continues to evolve",
    "continue to shape",
    "continues to shape",
    "remains a significant",
    "remains a critical",
    "offers a promising",
    "presents both opportunities and challenges",
    "both opportunities and challenges",
    "requires a comprehensive",
    "requires thoughtful",
    "driven by factors such as",
    "influenced by factors such as",
    "characterized by",
    "one of the most significant",
    "one of the most important",
    "one of the most pressing",
    "the importance of",
    "the significance of",
    "the role of",
    "addressing this challenge",
    "addressing these challenges",
    "addressing these issues",
    "multi-faceted approach",
    "holistic approach",
    "comprehensive approach",
    "for the benefit of",
    "a cornerstone of",
    "a fundamental",
    "far-reaching consequences",
    "far-reaching implications",
    "profound impact",
    "significant impact",
    "meaningful impact",
    "essential for",
    "crucial for",
    "vital for",
    "imperative that",
    "underscore the need",
    "underscores the need",
    "highlight the importance",
    "highlights the importance",
]

# Hedging phrases
_HEDGING_PHRASES = [
    "it should be noted",
    "one might argue",
    "it could be argued",
    "it can be said",
    "it may be",
    "it is possible that",
    "to some extent",
    "in some cases",
    "in many cases",
    "in certain cases",
    "it appears that",
    "it seems that",
    "arguably",
    "presumably",
    "perhaps",
    "potentially",
]


def _transition_word_density(words: list[str], sentences: list[str]) -> float:
    """
    AI texts overuse transition words. Count per sentence.
    Human: ~0.1-0.3 transitions/sentence  |  AI: ~0.5-1.0+
    """
    if len(sentences) < 3:
        return 0.5

    count = sum(1 for w in words if w in _TRANSITION_WORDS)
    density = count / len(sentences)

    # density >= 0.7 → very AI (1.0), density <= 0.15 → human (0.0)
    score = max(0.0, min(1.0, (density - 0.15) / 0.55))
    return score


def _formulaic_phrase_score(text: str, words: list[str]) -> float:
    """
    Count formulaic AI-typical phrases per 100 words.
    Higher density → more AI-like.
    """
    if len(words) < 30:
        return 0.5

    text_lower = text.lower()
    count = sum(1 for phrase in _FORMULAIC_PHRASES if phrase in text_lower)
    density = (count / len(words)) * 100  # per 100 words

    # density >= 3.0 → very AI (1.0), density <= 0.3 → human (0.0)
    score = max(0.0, min(1.0, (density - 0.3) / 2.7))
    return score


def _hedging_density(text: str, words: list[str]) -> float:
    """
    AI uses excessive hedging. Count per 100 words.
    """
    if len(words) < 30:
        return 0.5

    text_lower = text.lower()
    count = sum(1 for phrase in _HEDGING_PHRASES if phrase in text_lower)
    density = (count / len(words)) * 100

    score = max(0.0, min(1.0, (density - 0.1) / 1.5))
    return score


def _connective_overuse(words: list[str], sentences: list[str]) -> float:
    """
    Count discourse connectives per sentence.
    AI text leans heavily on these for structure.
    """
    connectives = frozenset({
        "however", "therefore", "consequently", "furthermore",
        "moreover", "nevertheless", "nonetheless", "thus",
        "hence", "accordingly", "meanwhile", "nonetheless",
        "although", "whereas", "despite", "regardless",
    })

    if len(sentences) < 3:
        return 0.5

    count = sum(1 for w in words if w in connectives)
    density = count / len(sentences)

    score = max(0.0, min(1.0, (density - 0.05) / 0.35))
    return score


# ═══════════════════════════════════════════════════════════
#  CATEGORY 2 — Statistical Patterns  (30 %)
# ═══════════════════════════════════════════════════════════

def _burstiness_score(sentences: list[str]) -> float:
    """
    Burstiness: measure sentence-length variance.
    Human text has HIGH variance, AI text has LOW variance.
    """
    if len(sentences) < 3:
        return 0.5

    lengths = [len(s.split()) for s in sentences]
    mean = sum(lengths) / len(lengths)
    if mean == 0:
        return 0.5

    variance = sum((l - mean) ** 2 for l in lengths) / len(lengths)
    cv = math.sqrt(variance) / mean

    # CV < 0.20 → very AI (1.0); CV > 0.60 → human (0.0)
    score = max(0.0, min(1.0, 1.0 - (cv - 0.20) / 0.40))
    return score


def _vocabulary_richness_mattr(words: list[str]) -> float:
    """
    MATTR (Moving Average Type-Token Ratio).
    AI tends to reuse vocabulary → lower TTR.
    """
    if len(words) < 30:
        return 0.5

    window = min(50, len(words))
    ttrs = []
    for i in range(len(words) - window + 1):
        chunk = words[i:i + window]
        ttr = len(set(chunk)) / len(chunk)
        ttrs.append(ttr)

    avg_ttr = sum(ttrs) / len(ttrs) if ttrs else 0.5

    # TTR < 0.50 → very AI (0.85); TTR > 0.78 → human (0.1)
    score = max(0.0, min(1.0, 1.0 - (avg_ttr - 0.50) / 0.28))
    return score


def _sentence_length_uniformity(sentences: list[str]) -> float:
    """
    Standard deviation of sentence word counts, normalized.
    AI text has more uniform sentence lengths.
    """
    if len(sentences) < 4:
        return 0.5

    lengths = [len(s.split()) for s in sentences]
    mean = sum(lengths) / len(lengths)
    if mean == 0:
        return 0.5

    std_dev = math.sqrt(sum((l - mean) ** 2 for l in lengths) / len(lengths))
    normalized = std_dev / mean

    # Low normalized std → uniform → AI-like
    score = max(0.0, min(1.0, 1.0 - (normalized - 0.15) / 0.40))
    return score


def _paragraph_regularity(text: str) -> float:
    """
    AI paragraphs tend to be similar in length.
    """
    paragraphs = [p.strip() for p in _PARA_SPLIT.split(text) if p.strip()]
    if len(paragraphs) < 2:
        return 0.5

    lengths = [len(p.split()) for p in paragraphs]
    mean = sum(lengths) / len(lengths)
    if mean == 0:
        return 0.5

    variance = sum((l - mean) ** 2 for l in lengths) / len(lengths)
    cv = math.sqrt(variance) / mean

    score = max(0.0, min(1.0, 1.0 - (cv - 0.10) / 0.45))
    return score


def _sentence_complexity(text: str, sentences: list[str]) -> float:
    """
    Commas per sentence. AI produces complex but uniform sentences.
    Very consistent comma usage → AI-like.
    """
    if len(sentences) < 3:
        return 0.5

    comma_counts = []
    for s in sentences:
        comma_counts.append(s.count(','))

    mean_commas = sum(comma_counts) / len(comma_counts)
    if mean_commas == 0:
        return 0.3

    # AI sweet spot: 1.5–3.0 commas/sentence, with low variance
    variance = sum((c - mean_commas) ** 2 for c in comma_counts) / len(comma_counts)
    cv = math.sqrt(variance) / mean_commas if mean_commas > 0 else 1.0

    # Low CV + moderate comma count → AI
    uniformity_score = max(0.0, min(1.0, 1.0 - (cv - 0.3) / 0.6))
    range_score = max(0.0, 1.0 - abs(mean_commas - 2.2) / 2.0)

    return 0.6 * uniformity_score + 0.4 * range_score


# ═══════════════════════════════════════════════════════════
#  CATEGORY 3 — Corpus Similarity    (20 %)
# ═══════════════════════════════════════════════════════════

def _corpus_tfidf_similarity(text: str) -> float:
    """
    Compare text against all reference corpus texts using TF-IDF cosine.
    High max similarity → text shares vocabulary/style with known AI.
    """
    from .reference_corpus import get_reference_texts

    ref_texts = get_reference_texts()
    if not ref_texts:
        return 0.5

    corpus_texts = [data["text"] for data in ref_texts.values()]
    all_docs = [text] + corpus_texts

    try:
        vectorizer = TfidfVectorizer(
            max_features=5000,
            ngram_range=(1, 2),
            stop_words="english",
        )
        tfidf_matrix = vectorizer.fit_transform(all_docs)
        similarities = sklearn_cosine(tfidf_matrix[0:1], tfidf_matrix[1:])
        max_sim = float(similarities.max())
    except (ValueError, Exception):
        return 0.5

    # max_sim >= 0.35 → likely AI style (1.0); max_sim <= 0.08 → human (0.0)
    score = max(0.0, min(1.0, (max_sim - 0.08) / 0.27))
    return score


def _ngram_corpus_overlap(text: str) -> float:
    """
    Bigram/trigram overlap ratio between the text and reference corpus.
    AI text shares more n-gram patterns with other AI text.
    """
    from .reference_corpus import get_reference_texts

    ref_texts = get_reference_texts()
    if not ref_texts:
        return 0.5

    def get_ngrams(txt: str, n: int) -> set[str]:
        w = txt.lower().split()
        return {" ".join(w[i:i + n]) for i in range(len(w) - n + 1)}

    doc_bigrams = get_ngrams(text, 2)
    doc_trigrams = get_ngrams(text, 3)

    if not doc_bigrams:
        return 0.5

    # Combine all corpus text
    combined_corpus = " ".join(data["text"] for data in ref_texts.values())
    corpus_bigrams = get_ngrams(combined_corpus, 2)
    corpus_trigrams = get_ngrams(combined_corpus, 3)

    bi_overlap = len(doc_bigrams & corpus_bigrams) / len(doc_bigrams) if doc_bigrams else 0
    tri_overlap = len(doc_trigrams & corpus_trigrams) / len(doc_trigrams) if doc_trigrams else 0

    combined = 0.5 * bi_overlap + 0.5 * tri_overlap

    # combined >= 0.25 → AI-like (1.0); combined <= 0.04 → human (0.0)
    score = max(0.0, min(1.0, (combined - 0.04) / 0.21))
    return score


# ═══════════════════════════════════════════════════════════
#  EXTRA — Repetitive Opener Detection
# ═══════════════════════════════════════════════════════════

def _repetitive_opener_ratio(sentences: list[str]) -> float:
    """
    Ratio of sentences starting with the same first word.
    AI reuses "The", "This", "These" etc. excessively.
    """
    if len(sentences) < 5:
        return 0.5

    starters = []
    for s in sentences:
        words = s.split()
        if words:
            starters.append(words[0].lower())

    if not starters:
        return 0.5

    freq = Counter(starters)
    most_common_count = freq.most_common(1)[0][1]
    repetition_ratio = most_common_count / len(starters)

    # ratio >= 0.45 → very AI (1.0); ratio <= 0.12 → human (0.0)
    score = max(0.0, min(1.0, (repetition_ratio - 0.12) / 0.33))
    return score


# ═══════════════════════════════════════════════════════════
#  MAIN DETECTOR
# ═══════════════════════════════════════════════════════════

# Weights organized by category  (sum ≈ 1.0)
_WEIGHTS = {
    # Category 1 — Linguistic Markers (50%)
    "transition_density":    0.14,
    "formulaic_phrases":     0.14,
    "hedging_density":       0.08,
    "connective_overuse":    0.07,
    "repetitive_openers":    0.07,
    # Category 2 — Statistical Patterns (30%)
    "burstiness":            0.08,
    "vocabulary_richness":   0.07,
    "sentence_uniformity":   0.06,
    "paragraph_regularity":  0.05,
    "sentence_complexity":   0.04,
    # Category 3 — Corpus Similarity (20%)
    "corpus_similarity":     0.12,
    "ngram_overlap":         0.08,
}


def detect_ai(text: str, use_corpus: bool = True) -> dict:
    """
    Analyse text and return an AI-likelihood assessment.

    Args:
        text:        The document text to analyse.
        use_corpus:  Whether to run corpus-based comparisons (default True).

    Returns dict with:
        ai_score (float):  0.0–100.0  (higher = more likely AI)
        verdict (str):     'likely_human' | 'uncertain' | 'likely_ai'
        features (dict):   individual feature scores (0.0–1.0)
    """
    text = text.strip()
    sents = _sentences(text)
    w = _words(text)

    # Category 1 — Linguistic Markers
    features = {
        "transition_density":   round(_transition_word_density(w, sents), 3),
        "formulaic_phrases":    round(_formulaic_phrase_score(text, w), 3),
        "hedging_density":      round(_hedging_density(text, w), 3),
        "connective_overuse":   round(_connective_overuse(w, sents), 3),
        "repetitive_openers":   round(_repetitive_opener_ratio(sents), 3),
    }

    # Category 2 — Statistical Patterns
    features.update({
        "burstiness":           round(_burstiness_score(sents), 3),
        "vocabulary_richness":  round(_vocabulary_richness_mattr(w), 3),
        "sentence_uniformity":  round(_sentence_length_uniformity(sents), 3),
        "paragraph_regularity": round(_paragraph_regularity(text), 3),
        "sentence_complexity":  round(_sentence_complexity(text, sents), 3),
    })

    # Category 3 — Corpus Similarity
    if use_corpus:
        features.update({
            "corpus_similarity": round(_corpus_tfidf_similarity(text), 3),
            "ngram_overlap":     round(_ngram_corpus_overlap(text), 3),
        })
    else:
        features.update({
            "corpus_similarity": 0.5,
            "ngram_overlap":     0.5,
        })

    # Weighted average
    raw_score = sum(features[k] * _WEIGHTS[k] for k in _WEIGHTS)
    ai_score = round(raw_score * 100, 1)

    # Verdict thresholds
    if ai_score >= 60:
        verdict = "likely_ai"
    elif ai_score >= 35:
        verdict = "uncertain"
    else:
        verdict = "likely_human"

    return {
        "ai_score": ai_score,
        "verdict": verdict,
        "features": features,
    }
